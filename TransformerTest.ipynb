{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvyINlKKwaOb"
      },
      "source": [
        "# **CSCI323 Group Assignment**\n",
        "\n",
        "This is our exploration of using Transformers instead of a Naive Bayes classifer as an approach to perform sentiment analysis on a dataset.  \n",
        "\n",
        "Our dataset has been truncated to exclude \"https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset/resolve/main/train_df.csv\", due to an error with the file which causes our model tuning to crash. This approximately halves the size of our dataset. However, as we will see, the transformer model is able to achieve very similar accuracy to our Naives Bayes model, while having far less data to train  on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EwqvriVKP3X"
      },
      "source": [
        "## **Data Preparation**\n",
        "\n",
        "### **Loading the dataset**\n",
        "\n",
        "Firstly, we load the data into a pandas dataframe concatenate them into a single dataset. Our dataset is a combination of 2 huggingface datasets consisting of online comments, each pre-labelled with either **Negative(0)**, **Neutral(1)** or **Positive(2)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NEQBqh63KXHN",
        "outputId": "f9937b8c-62ad-44b7-bee3-081810fe4182"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9235</td>\n",
              "      <td>getting cds ready for tour</td>\n",
              "      <td>1</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16790</td>\n",
              "      <td>MC, happy mother`s day to your mom ;).. love yah</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24840</td>\n",
              "      <td>A year from now is graduation....i am pretty s...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20744</td>\n",
              "      <td>because you had chips and sale w/o me</td>\n",
              "      <td>1</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6414</td>\n",
              "      <td>Great for organising my work life balance</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                               text  label sentiment\n",
              "0   9235                         getting cds ready for tour      1   neutral\n",
              "1  16790   MC, happy mother`s day to your mom ;).. love yah      2  positive\n",
              "2  24840  A year from now is graduation....i am pretty s...      0  negative\n",
              "3  20744              because you had chips and sale w/o me      1   neutral\n",
              "4   6414          Great for organising my work life balance      2  positive"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Load the dataset\n",
        "#Dataset 1\n",
        "url1 = [\n",
        "    # \"https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset/resolve/main/train_df.csv\",\n",
        "    \"https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset/resolve/main/test_df.csv\",\n",
        "    \"https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset/resolve/main/val_df.csv\"\n",
        "]\n",
        "\n",
        "df_data1 = pd.concat([pd.read_csv(url) for url in url1], ignore_index=True)\n",
        "\n",
        "#Dataset 2\n",
        "url2 = [\n",
        "    \"https://huggingface.co/datasets/mteb/tweet_sentiment_extraction/resolve/main/train.jsonl\",\n",
        "    \"https://huggingface.co/datasets/mteb/tweet_sentiment_extraction/resolve/main/test.jsonl\"\n",
        "]\n",
        "df_data2 = pd.concat([pd.read_json(url, lines=True) for url in url2], ignore_index=True)\n",
        "\n",
        "#Rename df_data2[label_text] to df_data2[sentiment]\n",
        "df_data2.rename(columns={'label_text': 'sentiment'}, inplace=True)\n",
        "\n",
        "#Combine both datasets\n",
        "df_data = pd.concat([df_data1, df_data2], ignore_index=True)\n",
        "\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z3G7FGDz9OC"
      },
      "source": [
        "## **Pre-processing**\n",
        "\n",
        "We use the **Natural Language Toolkit(NLTK)** which is a comprehensive library for working with human language data. NLTK provides useful tools for text processing such as tokenization and lemmitization. Here is a quick overview and explanation of the other imports used in this portion of the code:\n",
        "1. The **punkt** tonkenizer is used for dividing a text into a list of words or sentences. This is needed for tokenization tasks.\n",
        "2. A list of **stopwords** like *and*, *is*, *the*, etc is downloaded which is often removed from text data to focus on more meaningful words.\n",
        "3. The **WordNet** database provides a large dictionary of words and their meanings, synonyms and antonymns. This is used for lemmatization where words are reduced to their base forms.\n",
        "4. The **Open Multilingual WordNet** package allows access to WordNet in multiple languages. Our dataset is multilingual so this helps us in multilingual text processing.\n",
        "5. **Words** is a list of English words which is used to filter or validate tokens to ensure that they are real words.\n",
        "6. **word_tokenize** is a function that breaks down text into individual words.\n",
        "7. **WordNetLemmatizer** is a tool that reduces words to their base form, eg *running* to *run*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qRwnbnnKfq2",
        "outputId": "951c8236-f74b-4ba6-964e-c3b07be8f02e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ssyab\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\ssyab\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\ssyab\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\ssyab\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\ssyab\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\python312\\lib\\site-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "   ---------------------------------------- 0.0/431.4 kB ? eta -:--:--\n",
            "   -- ------------------------------------- 30.7/431.4 kB ? eta -:--:--\n",
            "   --- ----------------------------------- 41.0/431.4 kB 667.8 kB/s eta 0:00:01\n",
            "   --- ----------------------------------- 41.0/431.4 kB 667.8 kB/s eta 0:00:01\n",
            "   -------- ------------------------------ 92.2/431.4 kB 585.1 kB/s eta 0:00:01\n",
            "   ------------ ------------------------- 143.4/431.4 kB 778.5 kB/s eta 0:00:01\n",
            "   ------------------- ------------------ 225.3/431.4 kB 919.0 kB/s eta 0:00:01\n",
            "   ------------------------------ --------- 327.7/431.4 kB 1.1 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 409.6/431.4 kB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 431.4/431.4 kB 1.2 MB/s eta 0:00:00\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "#Data processing\n",
        "#Convert text to lowercase\n",
        "df_data['text'] = df_data['text'].str.lower()\n",
        "\n",
        "#Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    if isinstance(text, str):\n",
        "        return re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
        "    else:\n",
        "        return text  # or return an empty string: ''\n",
        "\n",
        "#Apply punctuation removal to the text column\n",
        "df_data['text'] = df_data['text'].apply(remove_punctuation)\n",
        "\n",
        "# Function to convert emojis to text\n",
        "def convert_emojis(text):\n",
        "    if isinstance(text, str):\n",
        "        return emoji.demojize(text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply the function to the 'text' column\n",
        "df_data['text'] = df_data['text'].apply(convert_emojis)\n",
        "\n",
        "#Function to remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    if isinstance(text, str):\n",
        "        tokens = word_tokenize(text)\n",
        "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "        return ' '.join(filtered_tokens)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "#Apply stop word removal to the text column\n",
        "df_data['text'] = df_data['text'].apply(remove_stop_words)\n",
        "\n",
        "#Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    if isinstance(text, str):\n",
        "        return word_tokenize(text)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "#Apply tokenization to the text column\n",
        "df_data['tokens'] = df_data['text'].apply(tokenize_text)\n",
        "\n",
        "#Function to lemmatize tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    if isinstance(tokens, list):\n",
        "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    else:\n",
        "        return tokens\n",
        "\n",
        "#Apply lemmatization to the tokens column\n",
        "df_data['lemmatized_tokens'] = df_data['tokens'].apply(lemmatize_tokens)\n",
        "\n",
        "#Join lemmatized tokens back into strings\n",
        "df_data['lemmatized_text'] = df_data['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "#Function to remove null values\n",
        "def remove_null_values(df, column_name):\n",
        "    df = df.dropna(subset=[column_name])\n",
        "    return df\n",
        "\n",
        "#Apply null value removal to the lemmatized text column\n",
        "df_data = remove_null_values(df_data, 'lemmatized_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D7WahyjwKsJ3"
      },
      "outputs": [],
      "source": [
        "#Join the lemmatized tokens back into a single string\n",
        "df_data['processed_text'] = df_data['lemmatized_tokens'].apply(lambda tokens: ' '.join(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFlquLD9Kn01"
      },
      "source": [
        "### **Vectorization and Train-Test Split**\n",
        "\n",
        "A vectorizer converts our text column into tokens and then converts the tokens into numerical vectors.  \n",
        "\n",
        "We imported **TfidVectorizer** which implements vectorization by converting text into a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) features. This matrix quantifies the importance of each word in a document relative to the entire corpus, helping to highlight words that are more relevant to each document.  \n",
        "\n",
        "Next, we will do the test-train split: 70% of the dataset will be used for training the model and the remaining 30% is used for testing. We imported the **train_test_split** function from scikit-learn module. This separation helps ensure that the model's performance metrics are reliable and that it doesn't overfit to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EbjCIKa8KuL1"
      },
      "outputs": [],
      "source": [
        "#Split the dataframe into inputs and expected outputs\n",
        "x = df_data['processed_text']\n",
        "y = df_data['label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Split x and y into training sets and test sets\n",
        "#Split the dataset into 70% training and 30% testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Initialize CountVectorizer and fit on training data\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "vectorizer.fit(x_train)\n",
        "\n",
        "#Transform the training and test data\n",
        "x_train_vectorized = vectorizer.transform(x_train)\n",
        "x_test_vectorized = vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maUP1ppGK-BO"
      },
      "source": [
        "## **Model Training**\n",
        "\n",
        "We fit the dataset onto the Multinomial Model and train the model on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGdMlbO7XzGI"
      },
      "source": [
        "### **Multinomial**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdWhRuUHK9Rd",
        "outputId": "b6c9d831-c75b-4cd3-cc61-689dd7b7cd88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultinomialNB model accuracy is 69.51%\n",
            "------------------------------------------------\n",
            "Confusion Matrix:\n",
            "      0     1     2\n",
            "0  1709  1708   163\n",
            "1   205  4291   446\n",
            "2    47  1220  2639\n",
            "------------------------------------------------\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.48      0.62      3580\n",
            "           1       0.59      0.87      0.71      4942\n",
            "           2       0.81      0.68      0.74      3906\n",
            "\n",
            "    accuracy                           0.70     12428\n",
            "   macro avg       0.76      0.67      0.69     12428\n",
            "weighted avg       0.74      0.70      0.69     12428\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Train a naive bayes classifier: MultinomialNB\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train_vectorized, y_train)\n",
        "\n",
        "from sklearn import metrics\n",
        "mnb_predicted = mnb.predict(x_test_vectorized)\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_test, mnb_predicted)\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(x_train_vectorized, y_train)\n",
        "\n",
        "print('MultinomialNB model accuracy is',str('{:04.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "print('------------------------------------------------')\n",
        "print('Confusion Matrix:')\n",
        "print(pd.DataFrame(confusion_matrix(y_test, mnb_predicted)))\n",
        "print('------------------------------------------------')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, mnb_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXW24GqfXzGM"
      },
      "source": [
        "## **Hyperparemeter Tuning**\n",
        "\n",
        "Here we train the model, using the same hyperparemeter grid that we used on our larger dataset for consistency. We then conduct model evaluation by calculating accuracy and ROC AUC score for the tuned model as a basis for comparison with our Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hskTHgKXLojE",
        "outputId": "267f7a38-e89c-49a9-d277-f91790c06d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'alpha': 0.5}\n",
            "Best Cross-Validation Score: 0.6841853530037877\n",
            "Best Hyperparameters: {'alpha': 0.5}\n",
            "Accuracy for multinomial model: 70.95268747988413%\n",
            "ROC AUC Score: 0.8730\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "#Perform GridSearchCV on multinomial model\n",
        "grid_search = GridSearchCV(mnb, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(x_train_vectorized, y_train)\n",
        "\n",
        "#Print the best parameters and best score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_}\")\n",
        "\n",
        "#Get the best model and hyperparameters\n",
        "best_clf = grid_search.best_estimator_\n",
        "print(f'Best Hyperparameters: {grid_search.best_params_}')\n",
        "\n",
        "#Make predictions on the test set with the best model\n",
        "predictionsMNB = best_clf.predict(x_test_vectorized)\n",
        "\n",
        "#Retrain the model using the best parameters\n",
        "MNB_best_model = grid_search.best_estimator_\n",
        "MNB_best_model.fit(x_train_vectorized, y_train)\n",
        "\n",
        "#Calculate the accuracy\n",
        "accuracyMNB = accuracy_score(y_test, predictionsMNB)\n",
        "print(f'Accuracy for multinomial model: {accuracyMNB * 100}%')\n",
        "\n",
        "#Calculate ROC AUC Score\n",
        "MNB_prob = best_clf.predict_proba(x_test_vectorized)\n",
        "MNB_roc_auc = roc_auc_score(y_test, MNB_prob, multi_class='ovr')\n",
        "print(f'ROC AUC Score: {MNB_roc_auc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y94yR0fULymo",
        "outputId": "6362c895-eece-477c-9eb3-756ae255692e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model: MNB\n",
            "ROC AUC Score: 0.8730\n",
            "Accuracy: 0.7095\n"
          ]
        }
      ],
      "source": [
        "#Set roc_auc to best model\n",
        "roc_auc = MNB_roc_auc\n",
        "\n",
        "#Determine the best model based on roc_auc\n",
        "best_model = \"MNB\"\n",
        "\n",
        "#Set predictions to best model\n",
        "predictions = globals()[f\"predictions{best_model}\"]\n",
        "\n",
        "#Set accuracy to best model\n",
        "accuracy = globals()[f\"accuracy{best_model}\"]\n",
        "\n",
        "print(f'Best Model: {best_model}')\n",
        "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lizyhfmhbxvd"
      },
      "source": [
        "## **Transformer Exploration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRKzDUQUXzGR",
        "outputId": "a281482a-4ba5-4ba6-fc89-130f026fecda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(MNB_best_model, 'naive_bayes_model.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7ZkPiztXzGR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDzUhur4XzGR"
      },
      "outputs": [],
      "source": [
        "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS_B7WOZXzGR",
        "outputId": "ceb1265f-9833-4c5a-b60a-d95a252c3212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7d1a3ea17550>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(x_train.tolist(), truncation=True, padding='max_length', max_length=514, return_tensors='pt')\n",
        "test_encodings = tokenizer(x_test.tolist(), truncation=True, padding='max_length', max_length=514, return_tensors='pt')\n",
        "\n",
        "# Convert to torch Dataset\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}  # Adjusted tensor creation\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, y_train.tolist())\n",
        "test_dataset = SentimentDataset(test_encodings, y_test.tolist())\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMwe5b2fXzGS"
      },
      "outputs": [],
      "source": [
        "# Function to calculate ROC AUC score and accuracy\n",
        "def compute_metrics(model, dataset):\n",
        "    trainer = Trainer(model=model)\n",
        "    predictions = trainer.predict(dataset)\n",
        "\n",
        "    # Apply softmax to convert logits to probabilities\n",
        "    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "    # Compute predicted classes\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "\n",
        "    # Compute ROC AUC score for multiclass classification\n",
        "    roc_auc = roc_auc_score(dataset.labels, probs, multi_class='ovr')\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(dataset.labels, preds)\n",
        "\n",
        "    return roc_auc, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "zLFheHuvXzGS",
        "outputId": "36e8a502-0d59-4de0-82c1-8a3f9101e691"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC AUC Score before fine-tuning: 0.848275680533778\n",
            "Accuracy before fine-tuning: 0.6901351786289025\n"
          ]
        }
      ],
      "source": [
        "# Calculate ROC AUC score and accuracy before fine-tuning\n",
        "roc_auc_before, accuracy_before = compute_metrics(model, test_dataset)\n",
        "print(f\"ROC AUC Score before fine-tuning: {roc_auc_before}\")\n",
        "print(f\"Accuracy before fine-tuning: {accuracy_before}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ehiS0iQXzGS",
        "outputId": "12cd9dd6-b30e-41dd-c3c1-5413d07df2f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    max_grad_norm=1.0\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "2c6lwfd_XzGc",
        "outputId": "7076cf3c-366f-4cde-87d5-ba345f93b577"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='336' max='1359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 336/1359 15:26 < 47:19, 0.36 it/s, Epoch 0.74/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1359' max='1359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1359/1359 1:06:29, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.471600</td>\n",
              "      <td>0.593001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.331800</td>\n",
              "      <td>0.598337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1359, training_loss=0.4668948587202868, metrics={'train_runtime': 3992.0374, 'train_samples_per_second': 21.792, 'train_steps_per_second': 0.34, 'total_flos': 2.2960210061348784e+16, 'train_loss': 0.4668948587202868, 'epoch': 2.9966923925027564})"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "zsIcqMqKXzGc",
        "outputId": "c5d542e8-7d3a-43e8-f4d3-486e47a895f6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-973a3b1f10b7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save only the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_weights_3_epoch.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# To load the weights later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = AutoModelForSequenceClassification.from_pretrained(MODEL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# Save only the model weights\n",
        "torch.save(model.state_dict(), 'model_weights_3_epoch.pth')\n",
        "\n",
        "# To load the weights later\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "# model.load_state_dict(torch.load('model_weights.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "HUGA_Z7VXzGd",
        "outputId": "db262905-5bf5-4369-c19b-bba4f652aaed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROC AUC Score after fine-tuning: 0.9242\n",
            "Accuracy after fine-tuning: 0.7988\n"
          ]
        }
      ],
      "source": [
        "# Calculate ROC AUC score and accuracy after fine-tuning\n",
        "roc_auc_after, accuracy_after = compute_metrics(model, test_dataset)\n",
        "print(f\"ROC AUC Score after fine-tuning: {roc_auc_after:.4f}\")\n",
        "print(f\"Accuracy after fine-tuning: {accuracy_after:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
